{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho-CmvcgNUqj"
      },
      "outputs": [],
      "source": [
        "!pip install wheel setuptools pip --upgrade\n",
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing"
      ],
      "metadata": {
        "id": "DjhH2Qhv75yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from openai import OpenAI\n",
        "import time\n",
        "import numpy as np\n",
        "import gzip\n",
        "import re\n",
        "\n",
        "\n",
        "def parse(path):\n",
        "  g = gzip.open(path, 'rb')\n",
        "  for l in g:\n",
        "    yield eval(l)\n",
        "\n",
        "def getDF(path):\n",
        "  i = 0\n",
        "  df = {}\n",
        "  for d in parse(path):\n",
        "    df[i] = d\n",
        "    i += 1\n",
        "  return pd.DataFrame.from_dict(df, orient='index')\n",
        "\n",
        "# Function to truncate the string to 20 words or less\n",
        "def truncate_to_20_words(s):\n",
        "    # Check if the input is a string\n",
        "    if isinstance(s, str):\n",
        "        words = s.split()\n",
        "        return ' '.join(words[:20])\n",
        "    else:\n",
        "        # Return the input unchanged if it's not a string\n",
        "        return s\n",
        "\n",
        "Beautydf = getDF('reviews_Beauty_5.json.gz')\n",
        "Beautymetadf = getDF('meta_Beauty.json.gz')\n",
        "\n",
        "# Apply the function to the column\n",
        "Beautymetadf['title'] = Beautymetadf['title'].apply(truncate_to_20_words)\n",
        "\n",
        "merged_df = pd.merge(Beautydf, Beautymetadf, on='asin', how='left')\n",
        "merged_df = merged_df.dropna(subset=['title','description','categories','brand'])\n",
        "merged_df = merged_df.groupby('title').filter(lambda x: x['asin'].nunique() == 1)\n",
        "\n",
        "# Filter groups by size and apply the function\n",
        "merged_df6 = (merged_df.groupby('reviewerID').filter(lambda x: len(x) >= 6))  # Keep only users with >= 6 purchase history\n",
        "\n",
        "beauty_df = merged_df6.reset_index(drop = True)\n",
        "all_items = list( beauty_df['title'].unique() )\n",
        "all_cand_items = list( beauty_df['title'].unique() )\n",
        "\n",
        "# # Or, you may try with a smaller subset\n",
        "# unique_users = beauty_df['reviewerID'].dropna().unique()[:10]\n",
        "# beauty_df = beauty_df[beauty_df['reviewerID'].isin(unique_users)]\n",
        "# all_items = list( beauty_df['title'].unique() )\n"
      ],
      "metadata": {
        "id": "mBRdR99u8a67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = ''\n",
        "client = OpenAI(api_key = API_KEY)\n",
        "model_id = 'gpt-3.5-turbo-0125' # 'gpt-4-0125-preview'\n",
        "\n",
        "\n",
        "for id in beauty_df['reviewerID'].unique():\n",
        "\n",
        "    user_df = beauty_df[ beauty_df['reviewerID'] == id ]\n",
        "    user_df = user_df.sort_values(by='unixReviewTime', ascending = True)\n",
        "\n",
        "    user_items = list( user_df['title'].unique() )\n",
        "\n",
        "    # keep last 15 items\n",
        "    user_items_applied = user_items[-15:]\n",
        "    break\n",
        "\n",
        "\n",
        "APE_prompt = f\"\"\"\n",
        "We will employ GPT-3.5 to perform personalized recommendation for Beauty Products, in which we will feed GPT-3.5 with a user's prior purchase history as well as a set of candidate items to select.\n",
        "The user's prior history consists a list of beauty products, each represented by its product title. The following can be an example for the user's prior purchase history: {user_items_applied}\n",
        "The candidate items are a list of beauty products, each also represented by its product title.\n",
        "\n",
        "Now, please propose a novel, detailed, and step-by-step algorithm to reformulate the user purchase history into a format that is most suitable for GPT-3.5.\n",
        "\"\"\"\n",
        "\n",
        "prompts = []\n",
        "prompt_performances = {}\n",
        "for i in range(7):\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model = model_id, temperature = 1.0,\n",
        "\n",
        "        messages=[{\"role\": \"system\", \"content\": 'Please come up with a very detailed, clear, and novel candidate prompt according to the narrative provided.'},\n",
        "                    {\"role\": \"user\", \"content\": APE_prompt}],\n",
        "        timeout = 1200)\n",
        "\n",
        "    candidate_prompt = completion.choices[0].message.content\n",
        "    prompts.append(candidate_prompt)\n",
        "    print(candidate_prompt)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujlQGROR6wdE",
        "outputId": "06c1cb51-e276-4e26-98f8-d99fae794807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To formulate the user's prior purchase history into a format suitable for GPT-3.5, we can employ a step-by-step algorithm that preprocesses and structures the data effectively. Here is a detailed process to achieve this:\n",
            "\n",
            "Step 1: Extract Keywords from Product Titles\n",
            "- Remove special characters, numbers, and unnecessary symbols from the product titles.\n",
            "- Tokenize the remaining words to extract meaningful keywords.\n",
            "- Utilize techniques like stemming or lemmatization to standardize the keywords.\n",
            "\n",
            "Step 2: Remove Stopwords and Irrelevant Terms\n",
            "- Eliminate common stopwords (e.g., 'and', 'the', 'for') and irrelevant terms that do not contribute to the context.\n",
            "- Exclude brand names, generic words like 'facial,' 'mask,' 'oil,' etc., as they might not be crucial for understanding preferences.\n",
            "\n",
            "Step 3: Group Similar Products\n",
            "- Cluster or group similar products together based on their keyword similarity.\n",
            "- Use techniques such as cosine similarity or hierarchical clustering to categorize related items.\n",
            "\n",
            "Step 4: Create Sequences or Sets\n",
            "- Construct sequences of products from the same group or category.\n",
            "- Order these sequences chronologically based on the user's purchase history.\n",
            "\n",
            "Step 5: Encode the Data\n",
            "- Encode the sequences of products and corresponding categories in a format understandable by GPT-3.5.\n",
            "- Use numeric or one-hot encoding for categories and embeddings for product sequences.\n",
            "\n",
            "Step 6: Data Augmentation (Optional)\n",
            "- Optionally, augment the data by introducing variations or combinations of product sequences to improve model performance.\n",
            "- Maintain the coherence and relevance of the augmented data to the user's preferences.\n",
            "\n",
            "Step 7: Curate User Context\n",
            "- Formulate a clear and concise user context that encapsulates the reformulated purchase history.\n",
            "- Include relevant information such as frequency of purchase, preferred categories, or recent interactions with beauty products.\n",
            "\n",
            "Step 8: Define Input Structure\n",
            "- Define a standardized input structure that integrates the user context and encoded sequences of prior purchases.\n",
            "- Format the input data in a consistent manner to facilitate GPT-3.5's understanding.\n",
            "\n",
            "By following this algorithm, we can effectively reformat the user's purchase history in a manner well-suited for GPT-3.5, enabling it to provide personalized recommendations for beauty products based on the user's preferences.\n",
            "\n",
            "Algorithm to Prepare User Purchase History for GPT-3.5 Recommendation Algorithm:\n",
            "\n",
            "Step 1: Tokenization & Cleaning\n",
            "- Tokenize each product title in the user's prior purchase history using whitespace as the delimiter.\n",
            "- Clean each token by removing special characters, numbers, and irrelevant information (e.g., brand names, packaging sizes).\n",
            "\n",
            "Step 2: Lemmatization\n",
            "- Lemmatize each cleaned token to convert it into its base or root form to ensure consistency in the text data.\n",
            "\n",
            "Step 3: Stopword Removal\n",
            "- Remove common stopwords (e.g., 'the', 'and', 'for') from the lemmatized tokens to focus on the essential keywords.\n",
            "\n",
            "Step 4: Entity Recognition\n",
            "- Use Named Entity Recognition (NER) to identify and extract relevant entities such as product categories (e.g., 'Facial Mask', 'Eyeshadow Palette') from the lemmatized tokens.\n",
            "\n",
            "Step 5: Synonym Expansion\n",
            "- Expand the entities by including synonyms or related terms to enrich the vocabulary for better understanding by GPT-3.5 (e.g., 'Facial Mask' -> 'Face Mask', 'Sheet Mask', 'Mask Pack').\n",
            "\n",
            "Step 6: Embedding\n",
            "- Convert the cleaned and expanded tokens into word embeddings using pre-trained language models like Word2Vec or GloVe to represent each word as a vector in a high-dimensional space.\n",
            "\n",
            "Step 7: Sentence Formation\n",
            "- Reconstruct the processed tokens into coherent sentences or phrases while maintaining the sequence of purchase history to provide context for GPT-3.5.\n",
            "\n",
            "Step 8: Input Formatting\n",
            "- Concatenate the formatted sentences representing the user's prior purchase history into a single input string separated by a special token or punctuation to clearly delineate individual items.\n",
            "\n",
            "Step 9: Final Input Creation\n",
            "- Format the prepared user purchase history data along with the candidate items into a structured input format suitable for GPT-3.5 recommendation engine, ensuring clear distinction between user history and candidate items for accurate recommendations.\n",
            "\n",
            "By following this algorithm, the user's purchase history can be transformed into a comprehensive and structured format that enhances GPT-3.5's capability to provide personalized beauty product recommendations based on the user's preferences and past purchases.\n",
            "\n",
            "Algorithm for Reformulating User Purchase History for GPT-3.5 Personalized Recommendation:\n",
            "\n",
            "Step 1: Classifying Cosmetic Entities\n",
            "- Explore a comprehensive database of beauty products to identify common categories and attributes within the product titles.\n",
            "- Group similar products under common categories such as skincare, makeup, haircare, tools, etc.\n",
            "- Extract key attributes like brand names, specific ingredients, product types (e.g., serum, mask, mascara), and sizes (e.g., 4 oz, 6 oz).\n",
            "\n",
            "Step 2: Embedding Purchase History\n",
            "- Create embeddings for each product title in the user's purchase history using pre-trained word embeddings (e.g., word2vec, GloVe) or transformers (e.g., BERT, GPT-3) to capture semantic similarities.\n",
            "- Aggregate embeddings for all products in the purchase history to create a representative vector for the user's beauty preferences.\n",
            "\n",
            "Step 3: Generating User Profile\n",
            "- Combine the aggregated vector from Step 2 with additional user-specific information like demographics, skin type, beauty concerns, and preferences.\n",
            "- Normalize and scale the combined vector to ensure equal contribution from different features.\n",
            "\n",
            "Step 4: Formatting Candidate Items\n",
            "- Preprocess the candidate items list by applying the same steps used for the user purchase history to encode product titles into vectors.\n",
            "- Ensure that candidate items cover a diverse range of beauty categories and brands to provide personalized recommendations across different product types.\n",
            "\n",
            "Step 5: Input Preparation for GPT-3.5\n",
            "- Construct input sequences for GPT-3.5 by concatenating the user profile vector with each candidate item vector individually.\n",
            "- Include a prompt that specifies the user's purchase history, user profile, and the candidate item for which the recommendation is sought.\n",
            "\n",
            "Step 6: Generating Personalized Recommendations\n",
            "- Feed the formatted inputs into the GPT-3.5 model to generate personalized recommendations for each candidate item based on the user's preferences and purchase history.\n",
            "- Leverage the model's language generation capabilities to provide insightful recommendations, incorporating user-specific preferences and compatibility with past purchases.\n",
            "\n",
            "Step 7: Post-processing and Ranking\n",
            "- Extract the generated recommendations from the GPT-3.5 outputs and rank them based on relevance to the user's preferences, diversity in product types, and conciseness.\n",
            "- Present the top-ranked recommendations to the user in a user-friendly format, along with rationales for each recommendation based on the user profile and purchase history.\n",
            "\n",
            "By following this algorithm, we can effectively reformulate the user's purchase history into a format suitable for GPT-3.5-powered personalized beauty product recommendations, enhancing the user's shopping experience and satisfaction with tailored product suggestions.\n",
            "\n",
            "**Prompt for Beauty Products Recommendation Algorithm using GPT-3.5**\n",
            "\n",
            "Given a user's prior purchase history and a list of candidate beauty products, we aim to reformulate the user purchase history to create a tailored prompt for GPT-3.5 to deliver personalized beauty product recommendations. \n",
            "\n",
            "**Algorithm Steps:**\n",
            "\n",
            "1. **User Purchase History Transformation:**\n",
            "   a. **Normalization:** Normalize the product titles in the user's purchase history by removing special characters, converting to lowercase, and handling any specific formatting inconsistencies.\n",
            "   b. **Tokenization:** Tokenize the normalized product titles to break them into individual words for better processing and semantic understanding.\n",
            "   c. **Deduplication:** Remove any duplicate entries within the user purchase history to avoid biasing the recommendations towards repetitive items.\n",
            "\n",
            "2. **Candidate Item Alignment:**\n",
            "   a. **Normalization:** Apply the same normalization procedure used for the user purchase history to each product title in the candidate items list.\n",
            "   b. **Tokenization:** Tokenize the normalized candidate product titles for a consistent representation across all items.\n",
            "\n",
            "3. **Prompt Generation:**\n",
            "   a. **Contextual Embedding:** Create an initial prompt template that includes a brief introduction to the recommendation task along with placeholders for user historical purchases and candidate items.\n",
            "   b. **User History Integration:** Populate the placeholder for the user purchase history within the prompt template with the preprocessed and deduplicated product titles.\n",
            "   c. **Candidate Item Inclusion:** Insert the tokenized candidate product titles within the prompt to inform GPT-3.5 about the available options for recommendation.\n",
            "\n",
            "4. **Prompt Customization:**\n",
            "   a. **Personalization:** Consider injecting personalized information such as user preferences, feedback, or any additional context that may enhance the recommendation relevance.\n",
            "   b. **Context Expansion:** Incorporate relevant details like product types, brands, or usage scenarios to provide comprehensive guidance for GPT-3.5.\n",
            "\n",
            "5. **Final Prompt Delivery:**\n",
            "   a. **Quality Assurance:** Review the generated prompt to ensure coherence, consistency, and relevancy in communicating the recommendation task.\n",
            "   b. **Integration with GPT-3.5:** Feed the finalized prompt into the GPT-3.5 model to leverage its natural language processing capabilities for generating personalized beauty product recommendations based on the user's purchase history and candidate items.\n",
            "\n",
            "By following these detailed steps for transforming the user's purchase history into an optimized prompt format, we can facilitate accurate and effective personalized beauty product recommendations using GPT-3.5.\n",
            "\n",
            "Algorithm Title: Personalized Beauty Products Recommendation Algorithm for GPT-3.5\n",
            "\n",
            "Step 1: Data Preprocessing\n",
            "    - Take the user's prior purchase history, represented as a list of beauty products' titles.\n",
            "    - Convert the list of product titles into a single string, separating each title by a unique token (e.g., \":::::\").\n",
            "    - Add a starting token at the beginning of the string (e.g., \"<|startoftext|>\") and an ending token at the end of the string (e.g., \"\n",
            "\n",
            "Prompt Algorithm: Personalized Beauty Product Recommendation using GPT-3.5\n",
            "\n",
            "Step 1: Data Preprocessing\n",
            "1.1. Tokenization: Tokenize the user's prior purchase history and candidate items by splitting each product title into individual words.\n",
            "1.2. Removing special characters: Eliminate special characters, such as '&amp;', from the tokens to ensure clean text data.\n",
            "1.3. Lowercasing: Convert all tokens to lowercase to maintain consistency.\n",
            "1.4. Removing stopwords: Filter out common stopwords like 'of', 'for', 'with', etc., as they do not provide relevant information for recommendation.\n",
            "1.5. Lemmatization: Reduce the tokens to their base form using lemmatization to standardize words (e.g., 'mascaras' to 'mascara').\n",
            "\n",
            "Step 2: Encoding User Purchase History\n",
            "2.1. Generate a context string comprising the reformatted user's prior purchase history, separating each product title by a distinct separator (e.g., '|').\n",
            "2.2. Start a new line and provide a brief introduction such as 'Based on your previous purchases, we recommend the following products:\\n'.\n",
            "2.3. Concatenate the context string with the properly formatted candidate items to create the input prompt for GPT-3.5.\n",
            "\n",
            "Step 3: Feed Data to GPT-3.5 for Personalized Recommendation\n",
            "3.1. Utilize the personalized recommendation task API of GPT-3.5 to provide the input prompt created in Step 2.\n",
            "3.2. Specify the model configuration, including parameters like temperature, max_tokens, and top_p to fine-tune the recommendation generation process.\n",
            "3.3. Receive the generated response from GPT-3.5, which should include recommended products based on the user's purchase history and the provided candidates.\n",
            "\n",
            "Step 4: Post-processing\n",
            "4.1. Extract and present the recommended products from the response generated by GPT-3.5.\n",
            "4.2. Optionally, conduct additional filtering or sorting based on relevance scores or any other criteria to enhance the quality of recommendations.\n",
            "4.3. Display the final list of personalized beauty product recommendations to the user in a user-friendly format.\n",
            "\n",
            "By following this algorithm, we can effectively leverage GPT-3.5 to provide personalized beauty product recommendations by converting the user's purchase history into a suitable format for contextual input.\n",
            "\n",
            "Algorithm for reformulating user purchase history for GPT-3.5 recommendation model:\n",
            "\n",
            "1. Create a dictionary mapping each unique beauty product title to a unique numerical identifier. This will help in encoding the data in a format suitable for GPT-3.5.\n",
            "\n",
            "2. Tokenize each beauty product title in the user's prior purchase history using a tokenizer compatible with GPT-3.5, such as the BPE (Byte Pair Encoding) tokenizer.\n",
            "\n",
            "3. Replace each tokenized beauty product title with its corresponding numerical identifier from the dictionary created in step 1.\n",
            "\n",
            "4. Pad or trim the tokenized sequences as necessary to ensure a consistent input length for GPT-3.5.\n",
            "\n",
            "5. Encode the reformulated user purchase history data into a format compatible with GPT-3.5 input requirements (e.g., JSON format).\n",
            "\n",
            "6. Additional information such as metadata about the user (e.g., user ID, age, gender) or contextual information (e.g., time of purchase, frequency of purchases) can be included in the input data to provide additional context for the recommendation model.\n",
            "\n",
            "7. Finally, this reformulated user purchase history data can be fed into GPT-3.5 along with the candidate items to generate personalized recommendations based on the user's past purchase behavior.\n",
            "\n",
            "By following this algorithm, the user's prior purchase history can be effectively transformed into a format suitable for GPT-3.5 recommendation model, enabling the model to provide accurate and personalized beauty product recommendations based on the user's preferences and past purchases.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Evaluation"
      ],
      "metadata": {
        "id": "Ho28OPJW8ra6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the system message\n",
        "system_msg = \"Please serve as a Recommender System on Beauty Products, based on user's prior purchase information provided.\"\n",
        "\n",
        "right_count = 0\n",
        "compressed_right_count = 0\n",
        "total = 0\n",
        "for id in beauty_df['reviewerID'].unique():\n",
        "\n",
        "    user_df = beauty_df[ beauty_df['reviewerID'] == id ]\n",
        "    user_df = user_df.sort_values(by='unixReviewTime', ascending = True)\n",
        "\n",
        "    user_items = list( user_df['title'].unique() )\n",
        "\n",
        "    # keep last 15 items\n",
        "    user_items_applied = user_items[-15:]\n",
        "\n",
        "    # randomly generate 99 negative items (exclude all purchased items) + 1 positive item\n",
        "    filtered_list = [x for x in all_cand_items if x not in user_items]\n",
        "    sampled_items = list( random.sample(filtered_list, 99) ) # sampled items may include ground truth item (remove)\n",
        "\n",
        "    sampled_items.append( user_items_applied[-1] )\n",
        "    random.shuffle(sampled_items)\n",
        "\n",
        "    target = user_items_applied[-1]\n",
        "\n",
        "\n",
        "    augmented_prompt = (\n",
        "            f\"Given the user has purchased the following items in chronological order: \"\n",
        "            f\"{user_items_applied[:-1]}; output a list of 10 items to recommend out of the following candidate items ONLY; do NOT explain anything, just output the items:\"\n",
        "            f\"\\n{sampled_items}\"\n",
        "        )\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "            model = model_id, temperature = 0,\n",
        "            messages=[{\"role\": \"system\", \"content\": system_msg},\n",
        "                        {\"role\": \"user\", \"content\": augmented_prompt}],\n",
        "            timeout = 1200)\n",
        "\n",
        "    pred = completion.choices[0].message.content\n",
        "\n",
        "    total += 1\n",
        "    if target in pred:\n",
        "        right_count += 1\n",
        "\n",
        "    # Perform ADE:\n",
        "    reformulation_prompt = f'Please thoroughly reformulate the user purchase history based on the following algorithm:\\n\\n{prompts[0]}\\n\\nUser purchase history to reformulate: {user_items_applied[:-1]}\\n\\nReturn the reformulation of the user purchase history ONLY.'\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "            model = model_id, temperature = 1.0,\n",
        "\n",
        "            messages=[{\"role\": \"system\", \"content\": 'Please reformulate the user purchase history to be much more informative and detailed based on the narrative provided.'},\n",
        "                        {\"role\": \"user\", \"content\": reformulation_prompt}],\n",
        "            timeout = 1200)\n",
        "\n",
        "    reformulated_history = completion.choices[0].message.content\n",
        "\n",
        "\n",
        "    compressed_prompt = (\n",
        "            f\"Given the user has purchased the following items in chronological order:\\n\\n\"\n",
        "            f\"{reformulated_history}\\n\\nOutput a list of 10 items to recommend out of the following 100 candidate items ONLY; do NOT explain anything, just output the items:\"\n",
        "            f\"\\n{sampled_items}\"\n",
        "        )\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "            model = model_id, temperature = 0,\n",
        "            messages=[{\"role\": \"system\", \"content\": system_msg},\n",
        "                        {\"role\": \"user\", \"content\": compressed_prompt}],\n",
        "            timeout = 1200)\n",
        "\n",
        "    compressed_pred = completion.choices[0].message.content\n",
        "\n",
        "    if target in compressed_pred:\n",
        "        compressed_right_count += 1\n",
        "\n",
        "    if total % 20 == 0 or total == beauty_df['reviewerID'].nunique():\n",
        "        print(f\"Accuracy: {right_count/total}\")\n",
        "        print(f\"Compressed Accuracy: {compressed_right_count/total}\")\n",
        "        print()\n",
        "\n"
      ],
      "metadata": {
        "id": "iVbMea_HQiZ0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}